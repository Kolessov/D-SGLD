{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian logistic Regression for a9a Dataset C-ULA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook deals with the Centralized Unadjasted Langevin Dynamics Algorithm(C-ULA). We are going to apply C-ULA for bayesian logistic regression. Thanks to such logistic regression one would like to solve problem of binary classification.\n",
    "\n",
    "Since we consider C-ULA , then we have to say, that we have the only agent that has own personal data. Thus, we have one datset $\\mathcal{D}$ = $\\{x_{i} , y_{i}\\}_{i=1}^{n}$, where $x_{i} \\in \\mathbb{R}^{d \\times 1}$ ,whereas $y_{i} \\in \\mathbb{R}$. For bayesian logistic regression one can create one linear model, which is composed in_features = d and out_features = 1(class). Then, we have d parameters of this linear model, let me denote as $w \\in \\mathbb{R}^{1 \\times d}$.\n",
    "\n",
    "We are used to solve logistic regression, maximizing likelihood by parameters of the model (MLE problem)\n",
    "$$ p(y_{i} | x_{i}, w) \\rightarrow \\max_{w}$$\n",
    "\n",
    "However, we should introduce prior knowledge of parameters (for instance Laplacian knowledge)\n",
    "$$ p(w) = \\frac{1}{2} e^{-|w|}$$\n",
    "\n",
    "Thus, according with the Bayes's rule:\n",
    "$$ p(w| (x_{1},y_{1}),...,(x_{n},y_{n})) \\sim p(w) \\prod_{i=1}^{n} p(y_{i}|x_{i},w)$$\n",
    "\n",
    "Then, one can outperform Poor Bayesian inference, in other words, we want to get mode of posterior.\n",
    "\n",
    "Let's move on to the terminuses of Energy function:\n",
    "$$ p(w|(x_{1},y_{1}),...,(x_{n},y_{n})) \\sim exp(-U(w)) \\implies U(w) = - \\log p(w|x_{1},..x_{n}) = - [\\log p(w) + \\sum_{i=1}^{n} \\log p(y_{i}|x_{i},w)]$$\n",
    "\n",
    "Thus , we would like to find the miminum of energy-based function $U(w)$\n",
    "$$ w(k+1) = w(k) - \\alpha(k)\\nabla_{w(k)} U(w(k)) +\\sqrt{2\\alpha(k)} v(k) $$\n",
    "\n",
    "Then Langevin algorithm (ULA):\n",
    "$$ w(k+1) = w(k) + \\alpha(k)  \\frac{n}{m}\\sum_{i=1}^{n}[ \\nabla_{w(k)} \\log p(y_{i}|x_{i},w(k))] + \\alpha(k)\\nabla_{w(k)} \\log p(w(k)) + \\sqrt{2\\alpha(k)} v(k)$$\n",
    "\n",
    "Where $v(k) \\sim \\mathcal{N}(o,I_{d})$, m is length of one batch\n",
    "\n",
    "Since our prior is equal to : $$p(w) = \\frac{1}{2} e^{-|w|} \\implies \\frac{\\partial}{\\partial w} \\log p(w) = - sign(w)$$\n",
    "Then:\n",
    "$$ w(k+1) = w(k) + \\alpha(k)  \\frac{n}{m}\\sum_{i=1}^{n}[ \\nabla_{w(k)} \\log p(y_{i}|x_{i},w(k))] - \\alpha(k) sign(w(k)) + \\sqrt{2\\alpha(k)} v(k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Upload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Ytr = load_svmlight_file('a9a.txt')\n",
    "# Ytr has type that equals numpy.ndarray\n",
    "# But Xtr has type that equals to scipy.sparse.csr.csr_matrix, so one has to transform it\n",
    "# Xtr.todense() to numpy.matrix, but Xtr.toarray() to numpy.array\n",
    "# Xtr = Xtr.toarray() to numpy.array\n",
    "Xtr = Xtr.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_num_samples = Xtr.shape[0]\n",
    "num_feat = Xtr.shape[1]\n",
    "train_num_samples, test_num_samples = 26050, 6511\n",
    "\n",
    "batch_size = 10\n",
    "run_size = 2\n",
    "num_epochs = 1\n",
    "\n",
    "# for optimization step\n",
    "a = 0.004\n",
    "b = 230\n",
    "gamma = 0.55\n",
    "device = 'cpu'\n",
    "\n",
    "# change -1 to 0\n",
    "index = (Ytr == -1) # true or False\n",
    "Ytr[index] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Make torch.utils.data.Dataset from numpy.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class a9aDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, labels, train_idx, test_idx, train=True):\n",
    "        \n",
    "        \"\"\"\n",
    "        data : Xtr\n",
    "        labels : Ytr\n",
    "        train_idx : indeces of train objects from all data\n",
    "        test_idx :  indeces of test objects from all data\n",
    "        \"\"\"\n",
    "        # numpy.ndarray supports data[some indices of data]\n",
    "        self.train = train\n",
    "\n",
    "        self.data = dict()\n",
    "        self.data['traindata'] = torch.from_numpy(data[train_idx, :]).float()\n",
    "        self.data['trainlabels'] = torch.squeeze(torch.from_numpy(labels[train_idx]).float())\n",
    "        self.data['testdata'] = torch.from_numpy(data[test_idx, :]).float()\n",
    "        self.data['testlabels'] = torch.squeeze(torch.from_numpy(labels[test_idx]).float())\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        if self.train:\n",
    "            return self.data['traindata'].shape[0]\n",
    "        else:\n",
    "            return self.data['testdata'].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if self.train:\n",
    "            data, target = self.data['traindata'][index], self.data['trainlabels'][index]\n",
    "        else:\n",
    "            data, target = self.data['testdata'][index], self.data['testlabels'][index]\n",
    "\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Optimizer of SGLD (optimization step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class sgld(torch.optim.Optimizer):\n",
    "    \n",
    "    def __init__(self, params, lr , num_batches, weight_decay = True, addnoise = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        params : model.parameters()\n",
    "        lr  = 0.01\n",
    "        num_batches = len(train_dataloader)\n",
    "        weight_decay: True\n",
    "        addnoise: True\n",
    "        \n",
    "        \"\"\"\n",
    "        # inheritance from torch.optim.Optimizer (14.01.2020)\n",
    "        defaults = dict(lr = lr, batch_weight = num_batches ,weight_decay = weight_decay, addnoise = addnoise)\n",
    "        super(sgld, self).__init__(params, defaults)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def step(self):\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            weight_decay = group['weight_decay']\n",
    "            batch_weight = group['batch_weight']\n",
    "\n",
    "            # p: there will be bias and weights of model (indices: 0,1)\n",
    "            # p.grad: gradient of loss by p\n",
    "            for p in group['params']:\n",
    "                \n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                # p.grad is gradient of loss function by p, i.e. (gradient of sum of log-likelihood)\n",
    "                d_p = batch_weight * p.grad.data\n",
    "                \n",
    "                # gradient of prior\n",
    "                if weight_decay:\n",
    "                    d_p.add_( - torch.sign(p.data))\n",
    "                \n",
    "                # normal noise add for Langevin MCMC\n",
    "                if group['addnoise']:\n",
    "                    \n",
    "                    # tensor.new(tensor.size()) is faster , than torch.empty_like()\n",
    "                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1) / np.sqrt(group['lr'])\n",
    "                    p.data.add_(-group['lr'], 0.5 * d_p + langevin_noise)\n",
    "                else:\n",
    "                    p.data.add_(-group['lr'], 0.5 * d_p)\n",
    "                    \n",
    "               \n",
    "\n",
    "        return loss # None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class a9a_model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,num_inputs):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(num_inputs, 1)\n",
    "        self._initialize_parameters()\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "        laplace_distr = torch.distributions.laplace.Laplace(loc = torch.tensor([0.0]),scale = torch.tensor([1.0]))\n",
    "        weights = laplace_distr.rsample(self.linear.weight.shape).reshape(1,-1)\n",
    "        bias = laplace_distr.rsample(self.linear.bias.shape).reshape(self.linear.bias.shape[0])\n",
    "        self.linear.weight = torch.nn.Parameter(weights)\n",
    "        self.linear.bias =  torch.nn.Parameter(bias)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_evaluation(net, data_loader, device = 'cpu'):\n",
    "    \n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, labels in data_loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        outputs = net(data)\n",
    "\n",
    "        predicted = (torch.sigmoid(outputs.squeeze()) >= 0.5).float()\n",
    "\n",
    "        correct += predicted.cpu().eq(labels.view_as(predicted.cpu())).sum().item()\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, iterno, a, b, gamma):\n",
    "    lr = a / ((b + iterno)**gamma)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accuracy_av = np.zeros((261, run_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Base code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCEWithLogitsLoss: $L = {l_{1},..,l_{n}} $ where $  l_{n} = - [y_{n} \\log \\sigma(x_{n}) + (1- y_{n}) \\log (1 - \\sigma(x_{n}))]$\n",
    "It is worth noticing, that the gradient of this loss function is coincided to the loss of log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10. Accuracy: 76.54738135463063\n",
      "Iteration: 20. Accuracy: 76.11733988634619\n",
      "Iteration: 30. Accuracy: 78.26754722776839\n",
      "Iteration: 40. Accuracy: 74.62755337121794\n",
      "Iteration: 50. Accuracy: 72.63093226846874\n",
      "Iteration: 60. Accuracy: 76.83919520810936\n",
      "Iteration: 70. Accuracy: 82.90585163569344\n",
      "Iteration: 80. Accuracy: 82.49116879127631\n",
      "Iteration: 90. Accuracy: 82.58332053448011\n",
      "Iteration: 100. Accuracy: 82.76762402088772\n",
      "Iteration: 110. Accuracy: 83.01336200276455\n",
      "Iteration: 120. Accuracy: 83.443403471049\n",
      "Iteration: 130. Accuracy: 80.21809245891568\n",
      "Iteration: 140. Accuracy: 82.90585163569344\n",
      "Iteration: 150. Accuracy: 83.19766548917217\n",
      "Iteration: 160. Accuracy: 82.75226539702042\n",
      "Iteration: 170. Accuracy: 82.92121025956074\n",
      "Iteration: 180. Accuracy: 82.32222392873598\n",
      "Iteration: 190. Accuracy: 80.64813392720012\n",
      "Iteration: 200. Accuracy: 82.27614805713408\n",
      "Iteration: 210. Accuracy: 84.08846567347565\n",
      "Iteration: 220. Accuracy: 82.21471356166488\n",
      "Iteration: 230. Accuracy: 83.75057594839502\n",
      "Iteration: 240. Accuracy: 83.90416218706804\n",
      "Iteration: 250. Accuracy: 80.5099063123944\n",
      "Iteration: 260. Accuracy: 83.36661035171248\n",
      "Iteration: 270. Accuracy: 77.97573337428966\n",
      "Iteration: 280. Accuracy: 83.30517585624328\n",
      "Iteration: 290. Accuracy: 83.68914145292582\n",
      "Iteration: 300. Accuracy: 81.87682383658424\n",
      "Iteration: 310. Accuracy: 80.07986484410996\n",
      "Iteration: 320. Accuracy: 83.02872062663185\n",
      "Iteration: 330. Accuracy: 83.15158961757027\n",
      "Iteration: 340. Accuracy: 83.33589310397788\n",
      "Iteration: 350. Accuracy: 84.07310704960835\n",
      "Iteration: 360. Accuracy: 80.14129933957918\n",
      "Iteration: 370. Accuracy: 83.3973275994471\n",
      "Iteration: 380. Accuracy: 82.95192750729535\n",
      "Iteration: 390. Accuracy: 82.56796191061281\n",
      "Iteration: 400. Accuracy: 82.96728613116265\n",
      "Iteration: 410. Accuracy: 74.76578098602366\n",
      "Iteration: 420. Accuracy: 81.27783750575948\n",
      "Iteration: 430. Accuracy: 83.4587620949163\n",
      "Iteration: 440. Accuracy: 79.51159576101982\n",
      "Iteration: 450. Accuracy: 83.64306558132391\n",
      "Iteration: 460. Accuracy: 75.94839502380587\n",
      "Iteration: 470. Accuracy: 83.16694824143757\n",
      "Iteration: 480. Accuracy: 83.55091383812011\n",
      "Iteration: 490. Accuracy: 83.5201965903855\n",
      "Iteration: 500. Accuracy: 84.04238980187375\n",
      "Iteration: 510. Accuracy: 83.33589310397788\n",
      "Iteration: 520. Accuracy: 78.12931961296268\n",
      "Iteration: 530. Accuracy: 80.540623560129\n",
      "Iteration: 540. Accuracy: 79.29657502687759\n",
      "Iteration: 550. Accuracy: 82.06112732299187\n",
      "Iteration: 560. Accuracy: 82.4604515435417\n",
      "Iteration: 570. Accuracy: 83.07479649823375\n",
      "Iteration: 580. Accuracy: 82.03041007525725\n",
      "Iteration: 590. Accuracy: 83.19766548917217\n",
      "Iteration: 600. Accuracy: 82.09184457072647\n",
      "Iteration: 610. Accuracy: 80.61741667946552\n",
      "Iteration: 620. Accuracy: 82.66011365381662\n",
      "Iteration: 630. Accuracy: 80.55598218399632\n",
      "Iteration: 640. Accuracy: 82.81369989248964\n",
      "Iteration: 650. Accuracy: 81.84610658884964\n",
      "Iteration: 660. Accuracy: 82.81369989248964\n",
      "Iteration: 670. Accuracy: 76.20949162954999\n",
      "Iteration: 680. Accuracy: 81.06281677161726\n",
      "Iteration: 690. Accuracy: 83.16694824143757\n",
      "Iteration: 700. Accuracy: 78.80509906312395\n",
      "Iteration: 710. Accuracy: 83.36661035171248\n",
      "Iteration: 720. Accuracy: 83.55091383812011\n",
      "Iteration: 730. Accuracy: 81.52357548763631\n",
      "Iteration: 740. Accuracy: 82.15327906619567\n",
      "Iteration: 750. Accuracy: 83.84272769159884\n",
      "Iteration: 760. Accuracy: 78.03716786975887\n",
      "Iteration: 770. Accuracy: 81.92289970818615\n",
      "Iteration: 780. Accuracy: 74.42789126094301\n",
      "Iteration: 790. Accuracy: 83.73521732452772\n",
      "Iteration: 800. Accuracy: 81.96897557978805\n",
      "Iteration: 810. Accuracy: 83.02872062663185\n",
      "Iteration: 820. Accuracy: 83.12087236983567\n",
      "Iteration: 830. Accuracy: 83.4280448471817\n",
      "Iteration: 840. Accuracy: 82.55260328674551\n",
      "Iteration: 850. Accuracy: 83.16694824143757\n",
      "Iteration: 860. Accuracy: 81.15496851482108\n",
      "Iteration: 870. Accuracy: 82.93656888342805\n",
      "Iteration: 880. Accuracy: 83.30517585624328\n",
      "Iteration: 890. Accuracy: 83.82736906773152\n",
      "Iteration: 900. Accuracy: 76.43987098755952\n",
      "Iteration: 910. Accuracy: 80.84779603747504\n",
      "Iteration: 920. Accuracy: 82.59867915834741\n",
      "Iteration: 930. Accuracy: 80.31024420211949\n",
      "Iteration: 940. Accuracy: 83.35125172784518\n",
      "Iteration: 950. Accuracy: 84.05774842574105\n",
      "Iteration: 960. Accuracy: 80.00307172477346\n",
      "Iteration: 970. Accuracy: 83.53555521425281\n",
      "Iteration: 980. Accuracy: 83.443403471049\n",
      "Iteration: 990. Accuracy: 83.13623099370297\n",
      "Iteration: 1000. Accuracy: 81.47749961603441\n",
      "Iteration: 1010. Accuracy: 81.44678236829981\n",
      "Iteration: 1020. Accuracy: 82.3990170480725\n",
      "Iteration: 1030. Accuracy: 80.66349255106742\n",
      "Iteration: 1040. Accuracy: 83.07479649823375\n",
      "Iteration: 1050. Accuracy: 81.55429273537091\n",
      "Iteration: 1060. Accuracy: 82.09184457072647\n",
      "Iteration: 1070. Accuracy: 78.92796805406236\n",
      "Iteration: 1080. Accuracy: 81.80003071724774\n",
      "Iteration: 1090. Accuracy: 80.14129933957918\n",
      "Iteration: 1100. Accuracy: 81.99969282752265\n",
      "Iteration: 1110. Accuracy: 81.30855475349409\n",
      "Iteration: 1120. Accuracy: 83.70450007679312\n",
      "Iteration: 1130. Accuracy: 82.67547227768392\n",
      "Iteration: 1140. Accuracy: 79.61910612809092\n",
      "Iteration: 1150. Accuracy: 83.13623099370297\n",
      "Iteration: 1160. Accuracy: 83.4587620949163\n",
      "Iteration: 1170. Accuracy: 81.89218246045154\n",
      "Iteration: 1180. Accuracy: 83.27445860850868\n",
      "Iteration: 1190. Accuracy: 80.31024420211949\n",
      "Iteration: 1200. Accuracy: 81.73859622177852\n",
      "Iteration: 1210. Accuracy: 82.09184457072647\n",
      "Iteration: 1220. Accuracy: 82.87513438795884\n",
      "Iteration: 1230. Accuracy: 83.3973275994471\n",
      "Iteration: 1240. Accuracy: 79.26585777914299\n",
      "Iteration: 1250. Accuracy: 83.5048379665182\n",
      "Iteration: 1260. Accuracy: 83.87344493933344\n",
      "Iteration: 1270. Accuracy: 82.59867915834741\n",
      "Iteration: 1280. Accuracy: 83.55091383812011\n",
      "Iteration: 1290. Accuracy: 83.81201044386422\n",
      "Iteration: 1300. Accuracy: 83.70450007679312\n",
      "Iteration: 1310. Accuracy: 82.4297342958071\n",
      "Iteration: 1320. Accuracy: 82.99800337889725\n",
      "Iteration: 1330. Accuracy: 82.15327906619567\n",
      "Iteration: 1340. Accuracy: 82.69083090155122\n",
      "Iteration: 1350. Accuracy: 81.66180310244202\n",
      "Iteration: 1360. Accuracy: 83.32053448011058\n",
      "Iteration: 1370. Accuracy: 81.92289970818615\n",
      "Iteration: 1380. Accuracy: 83.93487943480264\n",
      "Iteration: 1390. Accuracy: 81.60036860697282\n",
      "Iteration: 1400. Accuracy: 82.84441714022424\n",
      "Iteration: 1410. Accuracy: 82.4604515435417\n",
      "Iteration: 1420. Accuracy: 82.15327906619567\n",
      "Iteration: 1430. Accuracy: 81.3699892489633\n",
      "Iteration: 1440. Accuracy: 83.67378282905851\n",
      "Iteration: 1450. Accuracy: 83.67378282905851\n",
      "Iteration: 1460. Accuracy: 81.64644447857472\n",
      "Iteration: 1470. Accuracy: 83.78129319612962\n",
      "Iteration: 1480. Accuracy: 82.85977576409154\n",
      "Iteration: 1490. Accuracy: 83.71985870066042\n",
      "Iteration: 1500. Accuracy: 82.69083090155122\n",
      "Iteration: 1510. Accuracy: 83.99631393027185\n",
      "Iteration: 1520. Accuracy: 84.16525879281217\n",
      "Iteration: 1530. Accuracy: 84.36492090308708\n",
      "Iteration: 1540. Accuracy: 83.5048379665182\n",
      "Iteration: 1550. Accuracy: 84.14990016894487\n",
      "Iteration: 1560. Accuracy: 84.27276915988327\n",
      "Iteration: 1570. Accuracy: 83.73521732452772\n",
      "Iteration: 1580. Accuracy: 82.59867915834741\n",
      "Iteration: 1590. Accuracy: 82.52188603901091\n",
      "Iteration: 1600. Accuracy: 81.27783750575948\n",
      "Iteration: 1610. Accuracy: 83.56627246198741\n",
      "Iteration: 1620. Accuracy: 79.28121640301029\n",
      "Iteration: 1630. Accuracy: 84.14990016894487\n",
      "Iteration: 1640. Accuracy: 83.5201965903855\n",
      "Iteration: 1650. Accuracy: 80.66349255106742\n",
      "Iteration: 1660. Accuracy: 83.58163108585471\n",
      "Iteration: 1670. Accuracy: 83.90416218706804\n",
      "Iteration: 1680. Accuracy: 83.96559668253724\n",
      "Iteration: 1690. Accuracy: 83.59698970972201\n",
      "Iteration: 1700. Accuracy: 83.75057594839502\n",
      "Iteration: 1710. Accuracy: 81.60036860697282\n",
      "Iteration: 1720. Accuracy: 83.28981723237598\n",
      "Iteration: 1730. Accuracy: 83.82736906773152\n",
      "Iteration: 1740. Accuracy: 83.19766548917217\n",
      "Iteration: 1750. Accuracy: 82.92121025956074\n",
      "Iteration: 1760. Accuracy: 83.91952081093534\n",
      "Iteration: 1770. Accuracy: 84.28812778375058\n",
      "Iteration: 1780. Accuracy: 83.04407925049915\n",
      "Iteration: 1790. Accuracy: 83.53555521425281\n",
      "Iteration: 1800. Accuracy: 82.50652741514361\n",
      "Iteration: 1810. Accuracy: 83.5201965903855\n",
      "Iteration: 1820. Accuracy: 83.4741207187836\n",
      "Iteration: 1830. Accuracy: 83.87344493933344\n",
      "Iteration: 1840. Accuracy: 74.2896636461373\n",
      "Iteration: 1850. Accuracy: 83.73521732452772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1860. Accuracy: 82.53724466287821\n",
      "Iteration: 1870. Accuracy: 83.76593457226232\n",
      "Iteration: 1880. Accuracy: 82.69083090155122\n",
      "Iteration: 1890. Accuracy: 83.59698970972201\n",
      "Iteration: 1900. Accuracy: 83.15158961757027\n",
      "Iteration: 1910. Accuracy: 83.21302411303947\n",
      "Iteration: 1920. Accuracy: 83.78129319612962\n",
      "Iteration: 1930. Accuracy: 83.88880356320074\n",
      "Iteration: 1940. Accuracy: 83.3973275994471\n",
      "Iteration: 1950. Accuracy: 83.05943787436645\n",
      "Iteration: 1960. Accuracy: 83.35125172784518\n",
      "Iteration: 1970. Accuracy: 77.80678851174935\n",
      "Iteration: 1980. Accuracy: 83.18230686530487\n",
      "Iteration: 1990. Accuracy: 80.4638304407925\n",
      "Iteration: 2000. Accuracy: 83.5201965903855\n",
      "Iteration: 2010. Accuracy: 83.75057594839502\n",
      "Iteration: 2020. Accuracy: 83.71985870066042\n",
      "Iteration: 2030. Accuracy: 83.33589310397788\n",
      "Iteration: 2040. Accuracy: 83.67378282905851\n",
      "Iteration: 2050. Accuracy: 81.44678236829981\n",
      "Iteration: 2060. Accuracy: 83.95023805866994\n",
      "Iteration: 2070. Accuracy: 83.88880356320074\n",
      "Iteration: 2080. Accuracy: 82.58332053448011\n",
      "Iteration: 2090. Accuracy: 83.67378282905851\n",
      "Iteration: 2100. Accuracy: 82.89049301182614\n",
      "Iteration: 2110. Accuracy: 79.4962371371525\n",
      "Iteration: 2120. Accuracy: 83.78129319612962\n",
      "Iteration: 2130. Accuracy: 82.84441714022424\n",
      "Iteration: 2140. Accuracy: 81.64644447857472\n",
      "Iteration: 2150. Accuracy: 81.98433420365535\n",
      "Iteration: 2160. Accuracy: 82.26078943326678\n",
      "Iteration: 2170. Accuracy: 84.08846567347565\n",
      "Iteration: 2180. Accuracy: 83.10551374596837\n",
      "Iteration: 2190. Accuracy: 83.91952081093534\n",
      "Iteration: 2200. Accuracy: 82.56796191061281\n",
      "Iteration: 2210. Accuracy: 83.27445860850868\n",
      "Iteration: 2220. Accuracy: 82.30686530486868\n",
      "Iteration: 2230. Accuracy: 82.13792044232837\n",
      "Iteration: 2240. Accuracy: 83.16694824143757\n",
      "Iteration: 2250. Accuracy: 81.76931346951314\n",
      "Iteration: 2260. Accuracy: 83.15158961757027\n",
      "Iteration: 2270. Accuracy: 83.73521732452772\n",
      "Iteration: 2280. Accuracy: 81.67716172630932\n",
      "Iteration: 2290. Accuracy: 84.22669328828137\n",
      "Iteration: 2300. Accuracy: 79.51159576101982\n",
      "Iteration: 2310. Accuracy: 81.3392720012287\n",
      "Iteration: 2320. Accuracy: 83.58163108585471\n",
      "Iteration: 2330. Accuracy: 83.58163108585471\n",
      "Iteration: 2340. Accuracy: 80.34096144985409\n",
      "Iteration: 2350. Accuracy: 83.59698970972201\n",
      "Iteration: 2360. Accuracy: 83.68914145292582\n",
      "Iteration: 2370. Accuracy: 83.02872062663185\n",
      "Iteration: 2380. Accuracy: 83.4894793426509\n",
      "Iteration: 2390. Accuracy: 80.4331131930579\n",
      "Iteration: 2400. Accuracy: 83.02872062663185\n",
      "Iteration: 2410. Accuracy: 83.09015512210107\n",
      "Iteration: 2420. Accuracy: 83.36661035171248\n",
      "Iteration: 2430. Accuracy: 83.59698970972201\n",
      "Iteration: 2440. Accuracy: 83.65842420519121\n",
      "Iteration: 2450. Accuracy: 83.56627246198741\n",
      "Iteration: 2460. Accuracy: 81.76931346951314\n",
      "Iteration: 2470. Accuracy: 83.4894793426509\n",
      "Iteration: 2480. Accuracy: 83.53555521425281\n",
      "Iteration: 2490. Accuracy: 82.32222392873598\n",
      "Iteration: 2500. Accuracy: 82.70618952541852\n",
      "Iteration: 2510. Accuracy: 80.75564429427124\n",
      "Iteration: 2520. Accuracy: 82.3836584242052\n",
      "Iteration: 2530. Accuracy: 81.89218246045154\n",
      "Iteration: 2540. Accuracy: 82.78298264475502\n",
      "Iteration: 2550. Accuracy: 83.78129319612962\n",
      "Iteration: 2560. Accuracy: 83.18230686530487\n",
      "Iteration: 2570. Accuracy: 82.96728613116265\n",
      "Iteration: 2580. Accuracy: 83.07479649823375\n",
      "Iteration: 2590. Accuracy: 83.01336200276455\n",
      "Iteration: 2600. Accuracy: 83.22838273690677\n",
      "Iteration: 10. Accuracy: 77.13100906158809\n",
      "Iteration: 20. Accuracy: 79.63446475195822\n",
      "Iteration: 30. Accuracy: 78.88189218246045\n",
      "Iteration: 40. Accuracy: 73.73675318691446\n",
      "Iteration: 50. Accuracy: 81.29319612962678\n",
      "Iteration: 60. Accuracy: 78.14467823682998\n",
      "Iteration: 70. Accuracy: 82.81369989248964\n",
      "Iteration: 80. Accuracy: 79.25049915527569\n",
      "Iteration: 90. Accuracy: 81.00138227614806\n",
      "Iteration: 100. Accuracy: 76.99278144678237\n",
      "Iteration: 110. Accuracy: 79.64982337582552\n",
      "Iteration: 120. Accuracy: 80.09522346797726\n",
      "Iteration: 130. Accuracy: 80.12594071571188\n",
      "Iteration: 140. Accuracy: 81.01674090001536\n",
      "Iteration: 150. Accuracy: 82.3529411764706\n",
      "Iteration: 160. Accuracy: 81.67716172630932\n",
      "Iteration: 170. Accuracy: 80.72492704653664\n",
      "Iteration: 180. Accuracy: 77.76071264014745\n",
      "Iteration: 190. Accuracy: 82.06112732299187\n",
      "Iteration: 200. Accuracy: 82.07648594685917\n",
      "Iteration: 210. Accuracy: 82.21471356166488\n",
      "Iteration: 220. Accuracy: 82.50652741514361\n",
      "Iteration: 230. Accuracy: 79.3887267700814\n",
      "Iteration: 240. Accuracy: 81.67716172630932\n",
      "Iteration: 250. Accuracy: 78.89725080632775\n",
      "Iteration: 260. Accuracy: 82.67547227768392\n",
      "Iteration: 270. Accuracy: 82.12256181846107\n",
      "Iteration: 280. Accuracy: 82.475810167409\n",
      "Iteration: 290. Accuracy: 83.24374136077408\n",
      "Iteration: 300. Accuracy: 77.06957456611887\n",
      "Iteration: 310. Accuracy: 82.15327906619567\n",
      "Iteration: 320. Accuracy: 82.26078943326678\n",
      "Iteration: 330. Accuracy: 80.34096144985409\n",
      "Iteration: 340. Accuracy: 78.3289817232376\n",
      "Iteration: 350. Accuracy: 80.4484718169252\n",
      "Iteration: 360. Accuracy: 82.59867915834741\n",
      "Iteration: 370. Accuracy: 82.24543080939948\n",
      "Iteration: 380. Accuracy: 82.07648594685917\n",
      "Iteration: 390. Accuracy: 76.40915373982492\n",
      "Iteration: 400. Accuracy: 83.24374136077408\n",
      "Iteration: 410. Accuracy: 81.72323759791122\n",
      "Iteration: 420. Accuracy: 82.84441714022424\n",
      "Iteration: 430. Accuracy: 74.44324988481033\n",
      "Iteration: 440. Accuracy: 81.70787897404392\n",
      "Iteration: 450. Accuracy: 83.10551374596837\n",
      "Iteration: 460. Accuracy: 79.95699585317156\n",
      "Iteration: 470. Accuracy: 82.72154814928582\n",
      "Iteration: 480. Accuracy: 82.92121025956074\n",
      "Iteration: 490. Accuracy: 81.24712025802488\n",
      "Iteration: 500. Accuracy: 83.5201965903855\n",
      "Iteration: 510. Accuracy: 81.21640301029028\n",
      "Iteration: 520. Accuracy: 81.20104438642298\n",
      "Iteration: 530. Accuracy: 83.28981723237598\n",
      "Iteration: 540. Accuracy: 82.12256181846107\n",
      "Iteration: 550. Accuracy: 82.30686530486868\n",
      "Iteration: 560. Accuracy: 82.53724466287821\n",
      "Iteration: 570. Accuracy: 82.90585163569344\n",
      "Iteration: 580. Accuracy: 77.05421594225157\n",
      "Iteration: 590. Accuracy: 82.49116879127631\n",
      "Iteration: 600. Accuracy: 82.84441714022424\n",
      "Iteration: 610. Accuracy: 82.4604515435417\n",
      "Iteration: 620. Accuracy: 82.33758255260328\n",
      "Iteration: 630. Accuracy: 82.90585163569344\n",
      "Iteration: 640. Accuracy: 83.07479649823375\n",
      "Iteration: 650. Accuracy: 72.33911841499001\n",
      "Iteration: 660. Accuracy: 80.92458915681155\n",
      "Iteration: 670. Accuracy: 82.04576869912457\n",
      "Iteration: 680. Accuracy: 78.77438181538935\n",
      "Iteration: 690. Accuracy: 82.3529411764706\n",
      "Iteration: 700. Accuracy: 81.69252035017662\n",
      "Iteration: 710. Accuracy: 80.64813392720012\n",
      "Iteration: 720. Accuracy: 82.15327906619567\n",
      "Iteration: 730. Accuracy: 83.4894793426509\n",
      "Iteration: 740. Accuracy: 77.54569190600522\n",
      "Iteration: 750. Accuracy: 82.3682998003379\n",
      "Iteration: 760. Accuracy: 83.36661035171248\n",
      "Iteration: 770. Accuracy: 83.38196897557978\n",
      "Iteration: 780. Accuracy: 82.78298264475502\n",
      "Iteration: 790. Accuracy: 82.52188603901091\n",
      "Iteration: 800. Accuracy: 83.27445860850868\n",
      "Iteration: 810. Accuracy: 81.93825833205345\n",
      "Iteration: 820. Accuracy: 81.47749961603441\n",
      "Iteration: 830. Accuracy: 83.25909998464138\n",
      "Iteration: 840. Accuracy: 83.82736906773152\n",
      "Iteration: 850. Accuracy: 81.47749961603441\n",
      "Iteration: 860. Accuracy: 83.19766548917217\n",
      "Iteration: 870. Accuracy: 83.15158961757027\n",
      "Iteration: 880. Accuracy: 83.96559668253724\n",
      "Iteration: 890. Accuracy: 83.90416218706804\n",
      "Iteration: 900. Accuracy: 76.62417447396713\n",
      "Iteration: 910. Accuracy: 82.16863769006297\n",
      "Iteration: 920. Accuracy: 83.12087236983567\n",
      "Iteration: 930. Accuracy: 75.73337428966364\n",
      "Iteration: 940. Accuracy: 83.99631393027185\n",
      "Iteration: 950. Accuracy: 83.5201965903855\n",
      "Iteration: 960. Accuracy: 80.18737521118108\n",
      "Iteration: 970. Accuracy: 83.82736906773152\n",
      "Iteration: 980. Accuracy: 83.85808631546614\n",
      "Iteration: 990. Accuracy: 83.67378282905851\n",
      "Iteration: 1000. Accuracy: 83.58163108585471\n",
      "Iteration: 1010. Accuracy: 82.4143756719398\n",
      "Iteration: 1020. Accuracy: 82.81369989248964\n",
      "Iteration: 1030. Accuracy: 81.10889264321916\n",
      "Iteration: 1040. Accuracy: 83.22838273690677\n",
      "Iteration: 1050. Accuracy: 81.61572723084012\n",
      "Iteration: 1060. Accuracy: 83.4894793426509\n",
      "Iteration: 1070. Accuracy: 82.03041007525725\n",
      "Iteration: 1080. Accuracy: 76.83919520810936\n",
      "Iteration: 1090. Accuracy: 81.44678236829981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1100. Accuracy: 83.05943787436645\n",
      "Iteration: 1110. Accuracy: 83.02872062663185\n",
      "Iteration: 1120. Accuracy: 83.82736906773152\n",
      "Iteration: 1130. Accuracy: 83.55091383812011\n",
      "Iteration: 1140. Accuracy: 82.98264475502995\n",
      "Iteration: 1150. Accuracy: 82.82905851635694\n",
      "Iteration: 1160. Accuracy: 83.4741207187836\n",
      "Iteration: 1170. Accuracy: 83.22838273690677\n",
      "Iteration: 1180. Accuracy: 82.07648594685917\n",
      "Iteration: 1190. Accuracy: 83.36661035171248\n",
      "Iteration: 1200. Accuracy: 82.92121025956074\n",
      "Iteration: 1210. Accuracy: 82.26078943326678\n",
      "Iteration: 1220. Accuracy: 82.92121025956074\n",
      "Iteration: 1230. Accuracy: 83.22838273690677\n",
      "Iteration: 1240. Accuracy: 83.4894793426509\n",
      "Iteration: 1250. Accuracy: 83.5201965903855\n",
      "Iteration: 1260. Accuracy: 78.80509906312395\n",
      "Iteration: 1270. Accuracy: 83.96559668253724\n",
      "Iteration: 1280. Accuracy: 83.5048379665182\n",
      "Iteration: 1290. Accuracy: 83.75057594839502\n",
      "Iteration: 1300. Accuracy: 83.4280448471817\n",
      "Iteration: 1310. Accuracy: 83.09015512210107\n",
      "Iteration: 1320. Accuracy: 80.4023959453233\n",
      "Iteration: 1330. Accuracy: 83.33589310397788\n",
      "Iteration: 1340. Accuracy: 83.32053448011058\n",
      "Iteration: 1350. Accuracy: 83.53555521425281\n",
      "Iteration: 1360. Accuracy: 83.70450007679312\n",
      "Iteration: 1370. Accuracy: 82.3682998003379\n",
      "Iteration: 1380. Accuracy: 81.10889264321916\n",
      "Iteration: 1390. Accuracy: 81.52357548763631\n",
      "Iteration: 1400. Accuracy: 83.18230686530487\n",
      "Iteration: 1410. Accuracy: 82.4604515435417\n",
      "Iteration: 1420. Accuracy: 80.69420979880202\n",
      "Iteration: 1430. Accuracy: 83.13623099370297\n",
      "Iteration: 1440. Accuracy: 82.76762402088772\n",
      "Iteration: 1450. Accuracy: 83.12087236983567\n",
      "Iteration: 1460. Accuracy: 77.71463676854553\n",
      "Iteration: 1470. Accuracy: 81.4007064966979\n",
      "Iteration: 1480. Accuracy: 80.75564429427124\n",
      "Iteration: 1490. Accuracy: 82.30686530486868\n",
      "Iteration: 1500. Accuracy: 83.35125172784518\n",
      "Iteration: 1510. Accuracy: 83.68914145292582\n",
      "Iteration: 1520. Accuracy: 82.58332053448011\n",
      "Iteration: 1530. Accuracy: 80.17201658731378\n",
      "Iteration: 1540. Accuracy: 81.98433420365535\n",
      "Iteration: 1550. Accuracy: 83.16694824143757\n",
      "Iteration: 1560. Accuracy: 82.67547227768392\n",
      "Iteration: 1570. Accuracy: 81.64644447857472\n",
      "Iteration: 1580. Accuracy: 83.53555521425281\n",
      "Iteration: 1590. Accuracy: 83.91952081093534\n",
      "Iteration: 1600. Accuracy: 83.71985870066042\n",
      "Iteration: 1610. Accuracy: 80.67885117493472\n",
      "Iteration: 1620. Accuracy: 80.74028567040394\n",
      "Iteration: 1630. Accuracy: 83.58163108585471\n",
      "Iteration: 1640. Accuracy: 82.56796191061281\n",
      "Iteration: 1650. Accuracy: 78.31362309937029\n",
      "Iteration: 1660. Accuracy: 82.49116879127631\n",
      "Iteration: 1670. Accuracy: 80.4331131930579\n",
      "Iteration: 1680. Accuracy: 81.81538934111504\n",
      "Iteration: 1690. Accuracy: 83.65842420519121\n",
      "Iteration: 1700. Accuracy: 82.85977576409154\n",
      "Iteration: 1710. Accuracy: 82.55260328674551\n",
      "Iteration: 1720. Accuracy: 80.89387190907695\n",
      "Iteration: 1730. Accuracy: 83.25909998464138\n",
      "Iteration: 1740. Accuracy: 79.63446475195822\n",
      "Iteration: 1750. Accuracy: 83.09015512210107\n",
      "Iteration: 1760. Accuracy: 80.89387190907695\n",
      "Iteration: 1770. Accuracy: 82.96728613116265\n",
      "Iteration: 1780. Accuracy: 83.67378282905851\n",
      "Iteration: 1790. Accuracy: 83.21302411303947\n",
      "Iteration: 1800. Accuracy: 80.37167869758869\n",
      "Iteration: 1810. Accuracy: 83.16694824143757\n",
      "Iteration: 1820. Accuracy: 83.18230686530487\n",
      "Iteration: 1830. Accuracy: 82.78298264475502\n",
      "Iteration: 1840. Accuracy: 83.24374136077408\n",
      "Iteration: 1850. Accuracy: 83.4280448471817\n",
      "Iteration: 1860. Accuracy: 83.61234833358931\n",
      "Iteration: 1870. Accuracy: 83.65842420519121\n",
      "Iteration: 1880. Accuracy: 83.33589310397788\n",
      "Iteration: 1890. Accuracy: 83.4280448471817\n",
      "Iteration: 1900. Accuracy: 77.62248502534173\n",
      "Iteration: 1910. Accuracy: 82.84441714022424\n",
      "Iteration: 1920. Accuracy: 83.21302411303947\n",
      "Iteration: 1930. Accuracy: 82.4450929196744\n",
      "Iteration: 1940. Accuracy: 81.06281677161726\n",
      "Iteration: 1950. Accuracy: 83.5201965903855\n",
      "Iteration: 1960. Accuracy: 80.87851328520965\n",
      "Iteration: 1970. Accuracy: 74.59683612348334\n",
      "Iteration: 1980. Accuracy: 80.4638304407925\n",
      "Iteration: 1990. Accuracy: 83.4126862233144\n",
      "Iteration: 2000. Accuracy: 79.83412686223315\n",
      "Iteration: 2010. Accuracy: 80.38703732145599\n",
      "Iteration: 2020. Accuracy: 83.4587620949163\n",
      "Iteration: 2030. Accuracy: 77.92965750268776\n",
      "Iteration: 2040. Accuracy: 82.3682998003379\n",
      "Iteration: 2050. Accuracy: 81.92289970818615\n",
      "Iteration: 2060. Accuracy: 83.3973275994471\n",
      "Iteration: 2070. Accuracy: 83.30517585624328\n",
      "Iteration: 2080. Accuracy: 83.56627246198741\n",
      "Iteration: 2090. Accuracy: 81.75395484564584\n",
      "Iteration: 2100. Accuracy: 83.99631393027185\n",
      "Iteration: 2110. Accuracy: 84.10382429734295\n",
      "Iteration: 2120. Accuracy: 83.99631393027185\n",
      "Iteration: 2130. Accuracy: 82.13792044232837\n",
      "Iteration: 2140. Accuracy: 83.87344493933344\n",
      "Iteration: 2150. Accuracy: 79.98771310090616\n",
      "Iteration: 2160. Accuracy: 83.65842420519121\n",
      "Iteration: 2170. Accuracy: 83.76593457226232\n",
      "Iteration: 2180. Accuracy: 83.19766548917217\n",
      "Iteration: 2190. Accuracy: 82.99800337889725\n",
      "Iteration: 2200. Accuracy: 83.21302411303947\n",
      "Iteration: 2210. Accuracy: 83.88880356320074\n",
      "Iteration: 2220. Accuracy: 80.78636154200584\n",
      "Iteration: 2230. Accuracy: 83.78129319612962\n",
      "Iteration: 2240. Accuracy: 83.88880356320074\n",
      "Iteration: 2250. Accuracy: 79.20442328367378\n",
      "Iteration: 2260. Accuracy: 83.64306558132391\n",
      "Iteration: 2270. Accuracy: 81.52357548763631\n",
      "Iteration: 2280. Accuracy: 83.73521732452772\n",
      "Iteration: 2290. Accuracy: 81.53893411150361\n",
      "Iteration: 2300. Accuracy: 83.68914145292582\n",
      "Iteration: 2310. Accuracy: 83.5048379665182\n",
      "Iteration: 2320. Accuracy: 83.35125172784518\n",
      "Iteration: 2330. Accuracy: 82.3990170480725\n",
      "Iteration: 2340. Accuracy: 83.38196897557978\n",
      "Iteration: 2350. Accuracy: 83.70450007679312\n",
      "Iteration: 2360. Accuracy: 82.27614805713408\n",
      "Iteration: 2370. Accuracy: 82.95192750729535\n",
      "Iteration: 2380. Accuracy: 83.58163108585471\n",
      "Iteration: 2390. Accuracy: 82.69083090155122\n",
      "Iteration: 2400. Accuracy: 83.36661035171248\n",
      "Iteration: 2410. Accuracy: 82.78298264475502\n",
      "Iteration: 2420. Accuracy: 83.32053448011058\n",
      "Iteration: 2430. Accuracy: 83.21302411303947\n",
      "Iteration: 2440. Accuracy: 83.4126862233144\n",
      "Iteration: 2450. Accuracy: 80.5099063123944\n",
      "Iteration: 2460. Accuracy: 83.4741207187836\n",
      "Iteration: 2470. Accuracy: 82.23007218553218\n",
      "Iteration: 2480. Accuracy: 81.3853478728306\n",
      "Iteration: 2490. Accuracy: 83.68914145292582\n",
      "Iteration: 2500. Accuracy: 83.99631393027185\n",
      "Iteration: 2510. Accuracy: 83.91952081093534\n",
      "Iteration: 2520. Accuracy: 83.68914145292582\n",
      "Iteration: 2530. Accuracy: 82.13792044232837\n",
      "Iteration: 2540. Accuracy: 79.26585777914299\n",
      "Iteration: 2550. Accuracy: 83.5201965903855\n",
      "Iteration: 2560. Accuracy: 76.76240208877284\n",
      "Iteration: 2570. Accuracy: 83.38196897557978\n",
      "Iteration: 2580. Accuracy: 83.76593457226232\n",
      "Iteration: 2590. Accuracy: 83.88880356320074\n",
      "Iteration: 2600. Accuracy: 83.70450007679312\n"
     ]
    }
   ],
   "source": [
    "for run in range(0, run_size):\n",
    "    \n",
    "    #permutation of total num samples for train and test\n",
    "    total_idx = np.random.choice(total_num_samples, total_num_samples, replace=False)\n",
    "    train_idx = total_idx[:train_num_samples]\n",
    "    test_idx = total_idx[-test_num_samples:]\n",
    "\n",
    "    # Train and test data sets\n",
    "    train_data = a9aDataset(data=Xtr, labels=Ytr, train_idx=train_idx, test_idx=test_idx, train=True)\n",
    "    test_data = a9aDataset(data=Xtr, labels=Ytr, train_idx=train_idx, test_idx=test_idx, train=False)\n",
    "\n",
    "    # Train and test data loaders\n",
    "    train_dataloader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model and loss\n",
    "    model = a9a_model(num_inputs=num_feat).to('cpu')\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    \n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = sgld(model.parameters(), lr=0.01, weight_decay=True, num_batches=len(train_dataloader), addnoise=True)\n",
    "    iterno = 1\n",
    "    adjust_lr(optimizer, iterno=iterno, a=a, b=b, gamma=gamma)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------#\n",
    "    # Test accuracy before training\n",
    "    test_accuracy = [accuracy_evaluation(model, test_dataloader, device)]\n",
    "    iterlist = [iterno]\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "        model.train()\n",
    "        for data, labels in train_dataloader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            iterno += 1\n",
    "            adjust_lr(optimizer, iterno=iterno, a=a, b=b, gamma=gamma)\n",
    "\n",
    "            if iterno % 10 == 0:\n",
    "                acc = accuracy_evaluation(model, test_dataloader, device)\n",
    "                print('Iteration: {}. Accuracy: {}'.format(iterno, acc))\n",
    "                test_accuracy.append(acc)\n",
    "                iterlist.append(iterno)\n",
    "                \n",
    "    test_accuracy_av[:, run] = test_accuracy\n",
    "    \n",
    "mean_accuracy = np.mean(test_accuracy_av, axis=1)\n",
    "std_accuracy = np.std(test_accuracy_av, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fontsize = 5\n",
    "f1 = plt.figure()\n",
    "ax = f1.gca()\n",
    "plt.plot(np.asarray(iterlist), np.asarray(mean_accuracy),color='blue')\n",
    "ax.fill_between(np.asarray(iterlist), (np.asarray(mean_accuracy)-std_accuracy), (np.asarray(mean_accuracy)+std_accuracy), color='b', alpha=.3)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "ax.axhline(85,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
